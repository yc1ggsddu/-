{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142fb62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "import numpy as np\n",
    "import gensim\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import pickle\n",
    "from keras import layers\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffea9eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=100 #  在100个词后截断评论\n",
    "max_words=50000 # 考虑前10000个出现的词 发现有134658个word_index，故调整为50000\n",
    "embedding_dim=100\n",
    "Embedding_matrix=np.load(\"./Embedding/Embedding_matrix_bigdata_100.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb3c37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 100)     5000000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 100, 64)      31872       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100, 64)      0           gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Dense)           (None, 100, 1)       65          dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_1 (Softmax)             (None, 100, 1)       0           attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 100, 64)      0           softmax_1[0][0]                  \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 6400)         0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           204832      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            33          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,236,802\n",
      "Trainable params: 236,802\n",
      "Non-trainable params: 5,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras import Input\n",
    "input_ = Input(shape=(maxlen,))\n",
    "words = layers.Embedding(max_words,embedding_dim,input_length=maxlen)(input_)\n",
    "sen =  layers.GRU(64,return_sequences=True)(words)  #[b_size,maxlen,64] # 重新train，加入dropout？\n",
    "sen=layers.Dropout(0.5)(sen)\n",
    "#attention\n",
    "attention_pre = layers.Dense(1, name='attention_vec')(sen)   #[b_size,maxlen,1]\n",
    "attention_probs  = layers.Softmax()(attention_pre)  #[b_size,maxlen,1] \n",
    "attention_mul = layers.Lambda(lambda x:x[0]*x[1])([attention_probs,sen])\n",
    "output = layers.Flatten()(attention_mul)\n",
    "output = layers.Dense(32,activation=\"relu\")(output)\n",
    "output = layers.Dense(1, activation='sigmoid')(output)\n",
    "model = Model(inputs = input_ , outputs = output)\n",
    "model.layers[1].set_weights([Embedding_matrix])\n",
    "model.layers[1].trainable=False\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
