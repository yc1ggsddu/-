{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9a2563f",
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # 加入这行代码可以只使用cpu，对于自定义LSTM参数类训练，只使用cpu训练速度更快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "890133c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "import numpy as np\n",
    "import gensim\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import pickle\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e63eb3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=100 #  在100个词后截断评论\n",
    "max_words=50000 # 考虑前50000个出现的词 发现有321713个word_index，故调整为50000\n",
    "embedding_dim=200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ba77b7",
   "metadata": {},
   "source": [
    "import keras\n",
    "keras.backend.clear_session()\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "print('Number of devices: %d' % strategy.num_replicas_in_sync)  # 输出设备数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34272aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cut(comments):\n",
    "     words = jieba.cut(comments)\n",
    "     return ','.join(words)# 以逗号连接分割后的词\n",
    "#apply函数默认会按行应用word_cut函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc31ded",
   "metadata": {},
   "source": [
    "# 从txt文件中读取停止词\n",
    "def Get_stopwords(path):\n",
    "    stopwords=[]\n",
    "    with open(path,encoding='utf-8') as words:\n",
    "        stopwords.extend([i.strip() for i in words.readlines()])\n",
    "    return stopwords\n",
    "stopwords=Get_stopwords(\"./stopwords.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65efd2e3",
   "metadata": {},
   "source": [
    "def remove_stopwords(segmented_text):\n",
    "     #字符串转为列表\n",
    "     segmented_list = segmented_text.split(',')#根据逗号分隔符，将字符串转为列表\n",
    "     #去停用词\n",
    "     words_without_stopwords = []\n",
    "     for w in segmented_list:\n",
    "         if w not in stopwords:\n",
    "             words_without_stopwords.append(w)\n",
    "     return words_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c4122f3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_cut' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32140/1667452538.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcomments\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mcomments\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGet_comments_and_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./Origin_Comments/new_big.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32140/1667452538.py\u001b[0m in \u001b[0;36mGet_comments_and_data\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# 使用jieba库对comments进行语句切割\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Comment'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Comment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 不加astype可能出错\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Comment'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Comment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_cut\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m# 使用去停用词函数，去除comments中的停用词\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Comment'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Comment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_cut' is not defined"
     ]
    }
   ],
   "source": [
    "# 从文件中读取comments和对应标签\n",
    "def Get_comments_and_data(path):\n",
    "    data=pd.read_csv(path)\n",
    "    # 使用jieba库对comments进行语句切割\n",
    "    data['Comment'] = data['Comment'].astype(str) # 不加astype可能出错\n",
    "    data['Comment'] = data['Comment'].apply(word_cut)\n",
    "    # 使用去停用词函数，去除comments中的停用词\n",
    "    data['Comment'] = data['Comment'].apply(remove_stopwords)\n",
    "    # 提取评论及评价\n",
    "    comments=data['Comment'].values\n",
    "    labels=data['label'].values\n",
    "    # 转成列表\n",
    "    comments=comments.tolist()\n",
    "    labels=labels.tolist()\n",
    "    return [comments,labels]\n",
    "[comments,labels]=Get_comments_and_data(\"./Origin_Comments/new_big.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3e03a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除comments中的评论的''以及' '以及'\\n'\n",
    "def Remove_blanks(comments):\n",
    "    for com in range(len(comments)):\n",
    "        while ' ' in comments[com]:\n",
    "            comments[com].remove(' ')\n",
    "        while '' in comments[com]:\n",
    "            comments[com].remove('')\n",
    "        while '\\n' in comments[com]:\n",
    "            comments[com].remove('\\n')\n",
    "Remove_blanks(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f9e80e",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b86b5",
   "metadata": {},
   "source": [
    "np.save(\"./Comments.npy\",comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9d3942c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comments=np.load(\"./Comments_CUT.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5f5534b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['成本', '低廉', 'PPT', '电影', 'Nico', '生命', '中', '一年', '发生', '事', 'Nico', '歌配', '情节', '倒', '不算', '尴尬', '女猪', '演得', '不错', '电影', '中', '少见', '中老年', '女', '摇滚', '艺人', '形象', '念惜', '过往', '落魄', '潦倒', '观众', '再少', '挑', '居民楼', '草台', '场子', '煽动', '蹦', '哒', '坦然', '接受', '衰老', '变丑', '接受', '遗弃', '儿子', '后悔', '但小破', '巡演', '宅斗', '奸情', '搞', '搞']),\n",
       "       list(['传记片', '半真半假', '真实', '成功', '一半', '走进', '人物', '内心世界', '这部', '影片', '纪录片', '真实']),\n",
       "       list(['意大利', '房东', '记得', '嚣张', '岔开', '大腿', '抽着', '烟', '说', '计划', '优雅', '女人', '总', '念错', '名字', '劳拉', '记得', '黯然', '病床', '前', '问起', '孩子', '真', '幸运', '犹太', '经纪人', '记得', '洒脱', '约定', '伊', '维萨', '回来', '就录', '新专辑', '妮可', '举着', '话筒', '录', '生命', '嚣张', '黯然', '声音', '洒脱', '逃离', '布拉格', '人生', '舞台', '死', '光明节', '烛光', '中']),\n",
       "       ..., list(['额', '历史剧', '爱情', '剧', 'baby', '剧']),\n",
       "       list(['找', '一票', '俊', '男', '美女', '熟面孔', '演', '一出', '宫', '心计', '两个', '女神', '养眼', '服化', '良心', '剧情', '太', '偶像剧', '玛丽', '圣母', '白莲花', '安妮', '智商', '堪忧', '娱乐性', '不错']),\n",
       "       list(['集合', '两', '大美女', '拍', '失望'])], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "39612ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=np.load(\"./labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7334b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(data,labels):\n",
    "    indices=np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data=data[indices]\n",
    "    labels=labels[indices]\n",
    "    return [data,labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da411450",
   "metadata": {},
   "source": [
    "分析com中发现最长156，最短为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "abd1276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[data,labels]=shuffle_data(data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b4c36cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Trans_texts_to_seq(comments):\n",
    "    global maxlen,max_words\n",
    "    tokenizer=Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(comments)\n",
    "    sequences=tokenizer.texts_to_sequences(comments)\n",
    "    data=pad_sequences(sequences,maxlen=maxlen)\n",
    "    return [data,tokenizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8917be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13b35d61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[data,tokenizer]=Trans_texts_to_seq(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93c1c63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ...,  True, False,  True],\n",
       "       [ True,  True,  True, ...,  True, False,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False,  True],\n",
       "       [ True,  True,  True, ...,  True,  True, False]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1==data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf2fd926",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95d82c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ..., 17953,   203,   203],\n",
       "       [    0,     0,     0, ...,    25,   217,    38],\n",
       "       [    0,     0,     0, ...,    39, 20395,    13],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,   278,  5038,   278],\n",
       "       [    0,     0,     0, ..., 12366,  4517,    10],\n",
       "       [    0,     0,     0, ...,  6190,    11,   133]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae65c605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29999"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fca63180",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./data_50000_100.npy\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87437f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./different_tokenizer/tokenizer_50000_100.pickle', 'wb') as handle: \n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17aa3092",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./different_tokenizer/tokenizer_50000_100.pickle', 'rb') as handle: \n",
    "    tokenizer = pickle.load(handle) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "83f0cfcc",
   "metadata": {},
   "source": [
    "# 加载词向量文件\n",
    "# 腾讯AIlab中下载的tencent-ailab-embedding-zh-d100-v0.2.0-s.txt（小词典）\n",
    "# wv_from_text = gensim.models.KeyedVectors.load_word2vec_format(\"./tencent-ailab-embedding-zh-d200-v0.2.0-s.txt\", binary=False)\n",
    "# 如果每次都用上面的方法加载，速度非常慢，可以将词向量文件保存成bin文件，以后就加载bin文件，速度会变快\n",
    "# wv_from_text.init_sims(replace=True)\n",
    "# wv_from_text.save(\"./tencent-ailab-embedding-zh-d200-v0.2.0-s.txt\".replace(\".txt\", \".bin\"))\n",
    "# 之后可以用下面的方式加载词向量\n",
    "wv_from_text = gensim.models.KeyedVectors.load(\"./Pretrain_dic/tencent-ailab-embedding-zh-d100-v0.2.0-s.bin\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fde499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_embedding_matrix(path): # path中填写预训练词典\n",
    "    global embedding_dim,max_words,maxlen \n",
    "    global tokenizer,wv_from_text\n",
    "    global index\n",
    "    wv_from_text = gensim.models.KeyedVectors.load(path, mmap='r')\n",
    "    # 获得所有词\n",
    "    vocab = wv_from_text.key_to_index\n",
    "    word_index=tokenizer.word_index\n",
    "    embedding_matrix=np.zeros((max_words,embedding_dim))\n",
    "    for word,i in word_index.items():\n",
    "        if i<max_words:\n",
    "            idx=vocab.get(word)# 看是否在字典中\n",
    "            if idx is not None:# 若找不到索引，则默认为0\n",
    "                embedding_vector=wv_from_text.get_vector(word)\n",
    "                embedding_matrix[i]=embedding_vector\n",
    "            else:\n",
    "                embedding_matrix[i]=np.random.uniform(size=(maxlen,))\n",
    "    return embedding_matrix\n",
    "Embedding_matrix=Get_embedding_matrix(\"./Pretrain_dic/tencent-ailab-embedding-zh-d100-v0.2.0.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff4054ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0ebca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding_matrix[0]=np.random.uniform(size=(maxlen,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8595d59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3c46fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./Embedding/Embedding_matrix_50000_100_big.npy\",Embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4ee79e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding_matrix=np.load(\"./Embedding/Embedding_matrix_200.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab219dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "def Build_model():\n",
    "    model=models.Sequential()\n",
    "    model.add(layers.Embedding(max_words,embedding_dim,input_length=maxlen))\n",
    "    model.add(SeqSelfAttention(64,history_only=True,name='Attention'))\n",
    "    model.add(layers.LSTM(64))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1,activation='sigmoid'))\n",
    "    model.layers[0].set_weights([Embedding_matrix])\n",
    "    model.layers[0].trainable=False\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cf6a58",
   "metadata": {},
   "source": [
    "np.save(\"./labels.npy\",labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d775fbb6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 200)          10000000  \n",
      "_________________________________________________________________\n",
      "Attention (SeqSelfAttention) (None, None, 200)         25729     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                67840     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 10,093,634\n",
      "Trainable params: 93,634\n",
      "Non-trainable params: 10,000,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "6250/6250 [==============================] - 2196s 351ms/step - loss: 0.5997 - acc: 0.6695 - val_loss: 0.5687 - val_acc: 0.6973\n",
      "Epoch 2/20\n",
      "6250/6250 [==============================] - 2197s 352ms/step - loss: 0.5846 - acc: 0.6835 - val_loss: 0.5665 - val_acc: 0.6974\n",
      "Epoch 3/20\n",
      "6250/6250 [==============================] - 2195s 351ms/step - loss: 0.5792 - acc: 0.6886 - val_loss: 0.5640 - val_acc: 0.6996\n",
      "Epoch 4/20\n",
      "6250/6250 [==============================] - 2195s 351ms/step - loss: 0.5752 - acc: 0.6923 - val_loss: 0.5596 - val_acc: 0.7014\n",
      "Epoch 5/20\n",
      "6250/6250 [==============================] - 2196s 351ms/step - loss: 0.5720 - acc: 0.6949 - val_loss: 0.5592 - val_acc: 0.7039\n",
      "Epoch 6/20\n",
      "6250/6250 [==============================] - 2196s 351ms/step - loss: 0.5691 - acc: 0.6979 - val_loss: 0.5585 - val_acc: 0.7038\n",
      "Epoch 7/20\n",
      "6250/6250 [==============================] - 2197s 352ms/step - loss: 0.5665 - acc: 0.7001 - val_loss: 0.5551 - val_acc: 0.7059\n",
      "Epoch 8/20\n",
      "6250/6250 [==============================] - 2197s 351ms/step - loss: 0.5644 - acc: 0.7015 - val_loss: 0.5567 - val_acc: 0.7039\n",
      "Epoch 9/20\n",
      "6250/6250 [==============================] - 2196s 351ms/step - loss: 0.5626 - acc: 0.7029 - val_loss: 0.5611 - val_acc: 0.6988\n",
      "Epoch 10/20\n",
      "6250/6250 [==============================] - 2197s 351ms/step - loss: 0.5612 - acc: 0.7043 - val_loss: 0.5534 - val_acc: 0.7068\n",
      "Epoch 11/20\n",
      "6250/6250 [==============================] - 2196s 351ms/step - loss: 0.5594 - acc: 0.7058 - val_loss: 0.5514 - val_acc: 0.7094\n",
      "Epoch 12/20\n",
      "6250/6250 [==============================] - 2197s 351ms/step - loss: 0.5581 - acc: 0.7068 - val_loss: 0.5510 - val_acc: 0.7089\n",
      "Epoch 13/20\n",
      "6250/6250 [==============================] - 2195s 351ms/step - loss: 0.5567 - acc: 0.7079 - val_loss: 0.5499 - val_acc: 0.7103\n",
      "Epoch 14/20\n",
      "6250/6250 [==============================] - 2197s 352ms/step - loss: 0.5555 - acc: 0.7090 - val_loss: 0.5478 - val_acc: 0.7120\n",
      "Epoch 15/20\n",
      "6250/6250 [==============================] - 2197s 352ms/step - loss: 0.5547 - acc: 0.7095 - val_loss: 0.5518 - val_acc: 0.7076\n",
      "Epoch 16/20\n",
      "6250/6250 [==============================] - 2198s 352ms/step - loss: 0.5539 - acc: 0.7103 - val_loss: 0.5520 - val_acc: 0.7100\n",
      "Epoch 17/20\n",
      "5074/6250 [=======================>......] - ETA: 8:25 - loss: 0.5535 - acc: 0.7107"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "model=Build_model()\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])\n",
    "history=model.fit(data,\n",
    "          labels,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0647e400",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321713"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6b967c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "def K_fold_validation(data,labels):\n",
    "    k=5 # 5折交叉验证\n",
    "    num_samples=len(data)//k\n",
    "    shuffle_data(data,labels)\n",
    "    validation_scores=[]\n",
    "    num_epochs=10\n",
    "    for fold in range(k):\n",
    "        val_data=data[num_samples*fold:num_samples*(fold+1)]\n",
    "        val_labels=labels[num_samples*fold:num_samples*(fold+1)]\n",
    "        train_data=np.concatenate((data[:num_samples*fold],data[num_samples*(fold+1):]))\n",
    "        train_labels=np.concatenate((labels[:num_samples*fold],labels[num_samples*(fold+1):]))\n",
    "        model=Build_model()\n",
    "        model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])\n",
    "        model.fit(train_data,train_labels,epochs=num_epochs,batch_size=256)\n",
    "        loss,accuracy=model.evaluate(val_data,val_labels)\n",
    "        validation_scores.append(accuracy)\n",
    "    return sum(validation_scores)/k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98c7346",
   "metadata": {},
   "source": [
    "scores=K_fold_validation(data,labels)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3918a1",
   "metadata": {},
   "source": [
    " model=Build_model()\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5330e28",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "history=model.fit(x_train,y_train,\n",
    "                epochs=10,\n",
    "                 batch_size=128,\n",
    "                 validation_split=0.2)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict=history.history\n",
    "loss_values=history_dict['acc']\n",
    "val_loss_values=history_dict['val_acc']\n",
    "\n",
    "epoches=range(1,len(loss_values)+1)\n",
    "\n",
    "plt.plot(epoches,loss_values,'bo',label='Training acc')\n",
    "plt.plot(epoches,val_loss_values,'b',label='Validation acc')\n",
    "plt.title('Training and Validation Acc')\n",
    "plt.xlabel('Epoches')\n",
    "plt.ylabel('Acc')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
